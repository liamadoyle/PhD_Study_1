---
title: "Prisoner's Dilemma Game (Success)"
author: "Liam Doyle"
format: html
embed-resources: true
editor: visual
---

## Description

This document is a summary of the analyses performed for the final meta-analytic model conducted in Study 1 of my dissertation. This model examined the relationship between psychopathy and success in the Prisoner's Dilemma Game.

As there were no nested effect sizes (i.e., studies that contributed more than one effect size), a simple random-effects meta-analytic model was used to analyze the relationship.

## Loading Packages

All relevant packages loaded below.

```{r library, message=FALSE}
library(tidyverse)
library(meta)
library(metafor)
library(devtools)
library(esc)
library(effectsize)
library(psych)
library(dmetar)
library(readxl)
```

## Loading Data

Loading the excel spreadsheet with all study information.

```{r data import, message=FALSE}
PDG <- read_excel("PDG.xlsx")
```

## Recoding Variables as Factors

Recoding relevant variables (i.e., moderators in the analysis) as factors.

```{r recode}
PDG$Sample_Source <- factor(PDG$Sample_Source,
                                  levels = c(0, 1, 2, 3),
                                  labels = c("Community", "Student", "Inmate", "Mixed"))

PDG$Publication_Status <- factor(PDG$Publication_Status,
                                       levels = c(0, 1),
                                       labels = c("Unpublished", "Published"))

PDG$Measure_coded <- factor(PDG$Measure_coded,
                                       levels = c(0, 1, 2, 3, 4, 5, 6),
                                       labels = c("SRP", "PCL", "PPI", 
                                                  "LSRP", "TriPM", "SD3",
                                                  "DD"))

PDG$Iterated <- factor(PDG$Iterated,
                                       levels = c(0, 1),
                                       labels = c("One-shot", "Iterated"))

PDG$Perceived_Opponent <- factor(PDG$Perceived_Opponent,
                                 levels = c(0, 1),
                                 labels = c("No", "Yes"))

PDG$Social_role <- factor(PDG$Social_role,
                                       levels = c(0, 1),
                                       labels = c("Close member", "Distant member"))

PDG$Incentives <- factor(PDG$Incentives,
                               levels = c(0, 1, 2),
                               labels = c("Not incentivized", "Participation incentivized",
                                          "Performance incentivized"))

PDG$Experimental_Manipulation <- factor(PDG$Experimental_Manipulation,
                                        levels = c(0, 1),
                                        labels = c("No manipulation", 
                                                   "Manipulation"))

PDG$Feedback <- factor(PDG$Feedback,
                             levels = c(0, 1),
                             labels = c("No feedback", "Feedback"))

PDG$Strategy <- factor(PDG$Strategy,
                             levels = c(0, 1, 2, 3, 4, 5),
                             labels = c("Live opponent", "TFT", "TF2T",
                                        "Neutral/Seesaw", "Random",
                                        "Replicator"))

PDG$Leniency <- factor(PDG$Leniency,
                             levels = c(0, 1, 2),
                             labels = c("Lenient", "Neutral", "Harsh"))
```

## Computing Additional Variables

Performing mathematical computations to compute relevant variables such as Fisher's *z*, as well as it's standard error and variance.

```{r transform, eval=FALSE}
PDG <- PDG %>%
  mutate(z_psyc_coop = 0.5 * log((1 + Psych_Coop)/(1 - Psych_Coop)))

PDG <- PDG %>%
  mutate(z_se_psyc_coop = 1/(sqrt(n_effect - 3)))

PDG <- PDG %>%
  mutate(var_z_psyc_coop = (z_se_psyc_coop^2))

PDG <- PDG %>%
  mutate(z_psyc_succ = 0.5 * log((1 + Psychopathy_Succ)/(1 - Psychopathy_Succ)))

PDG <- PDG %>%
    mutate(z_se_psyc_succ = (z_psyc_succ > 0), 1/(sqrt(n_effect - 3)))

PDG <- PDG %>%
  mutate(var_z_psyc_succ = (z_se_psyc_succ^2))
```

## Random-Effects Model of Psychopathy on Success in the PDG

As there were no nested effect sizes, a simple random-effects model was constructed. I have made the decision to use a random-effects model for the meta-analyses in this dissertation. The fixed-effects model makes untenable assumptions about the sources of variance in the effect sizes (i.e., assuming that all effect sizes are from a homogenous population, and the only source of variance in effect sizes is from sampling error). The assumptions of a fixed-effects model also stand in contradiction to the intended moderator analyses which make up a substantial portion of our later analyses (i.e., that between-study heterogeneity exists).

### Creation of the Model

```{r model}
PDG.remove.na <- drop_na(PDG, Psychopathy_Succ)

psyc.success <- metacor(cor = Psychopathy_Succ, 
                 n = n_effect,
                 studlab = Study,
                 data = PDG.remove.na,
                 fixed = FALSE,
                 random = TRUE,
                 method.tau = "REML",
                 method.random.ci = "HK",
                 title = "Psychopathy and Success in the PDG")

summary(psyc.success)
```

As we can see, the model estimates that the meta-analytic effect size is *r* = -.078, 95% CI \[-.046, .200\]. This effect size was not significant, *p* = .175. Cochran's *Q* statistic was not significant (*p* = .120), indicating that there is no between-study heterogeneity. However, the *I*^2^ value indicates that 40.7% of the variance in effect sizes is due to between-study heterogeneity (i.e., a moderate amount of between-study heterogeneity). The \$\\tau\$^2^ value is also non-zero, indicating some variance in the true effect sizes (although one should note that the 95% CI for this parameter estimate does include zero).

### Forest Plot of the Model

```{r forest plot, echo=FALSE, warning=FALSE}
forestplot <- meta::forest(psyc.success,
                           sortvar = Psychopathy_Succ,
                           prediction = TRUE,
                           print.tau2 = TRUE)
```

## Tests of Publication Bias

In this section, I examined the possibility of publication bias via the visual examination of funnel plots, Egger's regression test, a simple meta-regression which regressed publication status (as a binary factor) on effect size, and PET-PEESE methods.

### Funnel Plot

The funnel plot does indicate some asymmetry, with few small studies finding negative results. Again, however, asymmetry in a funnel plot has several possible sources (Page et al., 2020):

-   Publication bias and other reporting biases;
-   Variability in methodological quality;
-   Between-study heterogeneity;
-   Artefactual variance (i.e., via the correlation between effect sizes and their standard error)
-   Chance

```{r funnel}
meta::funnel(psyc.success,
             xlim = c(-1, 1),
             studlab = FALSE)

title("Funnel Plot of the Effect Sizes for the Psychopathy-Success Model")
```

### Egger's Regression Test

Egger's regression test (Egger et al., 1997) is a method that provides a statistical test of asymmetry in a funnel plot. It involves the prediction of standardized effect sizes on their precision.

The intercept (i.e., $\beta_0$) is of primary interest, representing the asymmetry of the plot. NHST can be used to test the null hypothesis that the intercept is equal to zero, indicating symmetry in the funnel plot. Conversely, a rejected null hypothesis for the intercept indicates asymmetry in the funnel plot - potentially due to publication bias.

The slope (i.e., $\beta_1$) represents the effect size adjusted for precision. It is expected to be consistent with the overall meta-analytic effect.

Reported below is the significance test and parameter estimate for the intercept.

```{r egger}
metabias(psyc.success,
         method.bias = "linreg",
         k.min = 5)
```

The intercept was not significant, indicating that there is not a substantial amount of asymmetry in the funnel plot.

### Publication Status as a Moderator of Effect Size

Simple meta-regression was used to predict effect sizes from publication status as a factor (i.e., 0 = unpublished, 1 = published). It should be noted that this is a provisional analysis, as meta-regression should not be used when there are fewer than 10 studies within a meta-analytic model (Deeks et al., 2023).

```{r moderator publication status}
mod.model.pub <- rma(yi = z_psyc_succ,
                           vi = var_z_psyc_succ,
                           data = PDG.remove.na,
                           method = "REML",
                           mods = ~ Publication_Status)

summary(mod.model.pub)
```

The omnibus test indicates that there is no significant difference between published and unpublished studies, *QM*(1) = 0.009, *p* = .924.

### PET & PEESE Analyses

The PET (Precision-Effect Test) and PEESE (Precision-Effect Estimate with Standard Error) are two analytical methods designed to address publication bias in meta-analyses.

The former, PET analysis, is designed to determine the degree to which small-study effects and publication bias impact a meta-analysis. To do so, effect sizes are predicted from their standard errors. This analysis is used to provide evidence that a genuine effect exists beyond the effect of small-study effects/publication bias.

#### PET Analysis

The key element of the PET analysis is the estimate for $\beta_0$ or the intercept of the regression. According to the logic for PET analysis, a non-significant intercept indicates that no genuine effect occurs beyond small-study effects/publication bias. In contrast, a significant intercept provides evidence for a true effect.

That being said, one potential issue with both PET and PEESE analysis is the assumption that there is a homogenous distribution underlying effect sizes. Conceptually, this is similar to the assumption underlying fixed-effects meta-analyses (https://willgervais.com/blog/2015/6/25/putting-pet-peese-to-the-test-1). While this is problematic in and of itself, we will proceed using the PET-PEESE method.

```{r PET}
pet_model <- rma(yi = z_psyc_succ,
                     vi = var_z_psyc_succ,
                    mods = ~ z_se_psyc_succ,
                     data = PDG.remove.na,
                     method = "REML")

summary(pet_model)
```

As seen in the output, the intercept in the model is not significant (*p* = .485). This indicates that no true effect exists beyond publication bias. While analysis would normally stop here, it must be noted, again, that this is a provisional analysis that goes against contemporary recommendations for the use of meta-regression when a model has fewer than 10 studies (Deeks et al., 2023). As the PET analysis is exploratory, we will continue to PEESE analysis.

#### PEESE Analysis

PEESE analysis aims to produce a precise estimate of the true effect size after accounting for sampling error, small-study effects, and publication bias. It does so by creating a regression where effect sizes are predicted by the squared standard errors of studies.

Again, the primary parameter estimate of interest is $\beta_0$, which indicates the estimate of the true effect size (after adjusting for sampling error, small-study effects, and publication bias). Significance tests are interpreted such that a significant intercept indicates a significant meta-analytic effect size.

A secondary parameter estimate of interest is $\beta_1$, which indicates the relationship between effect sizes and their variances. When this is significant, there is evidence that effect sizes are systematically related to the estimate of their precision (i.e., their variance), which may suggest the presence of small-study effects or publication bias.

More particularly, a positive slope indicates that larger variances (with larger standard errors, and, oftentimes, smaller sample sizes) report larger effect sizes. This is representative of the typical "small-study effect" and indicates potential publication bias.

In contrast, a negative slope suggests that smaller studies report smaller effect sizes and larger studies report larger effect sizes. This may indicate other sources of bias or sources of heterogeneity impacting the results.

```{r PEESE}
peese_model <- rma(yi = z_psyc_succ,
                     vi = var_z_psyc_succ,
                    mods = ~ var_z_psyc_succ,
                     data = PDG.remove.na,
                     method = "REML")

summary(peese_model)

convert_z2r(.0071)
```

As seen in the model output, the PEESE method indicates that the true effect size is *r* = .007 and non-significant at *p* = .934. This differs substantially from the model estimate of *r* = .078. Again, meta-regression should not be used in general when *k* \< 10 and the PET-PEESE method has been acknowledged to (a) perform poorly under conditions of small sample size (i.e., *k* \< 20) and high between-study heterogeneity (i.e., *I*^2^ \> 80) and (b) heavily over-corrects for bias in the data (Carter et al., 2019).

The slope coefficient indicates a non-significant, positive relationship between effect sizes and their variances (see above). Interestingly, the non-significance of the slope indicates that publication bias is not large enough to have affected the results.

To illustrate the slope estimated by the model visually, we will plot the regression.

```{r PEESE plot}
plot(
  PDG.remove.na$var_z_psyc_succ, PDG.remove.na$z_psyc_succ,
  xlab = "Variance of Effect Sizes",
  ylab = "Effect Sizes",
  main = "PEESE Analysis of Psychopathy-Success Meta-Analysis (PDG)"
)
abline(a = coef(peese_model)[1], b = coef(peese_model)[2], col = "blue")
```

### Fail-Safe *N*

A variation of Orwin's (1983) fail-safe N test was used to indicate the number of studies with null effects needed to reduce the effect size to a practically irrelevant size (Viechtbauer, 2010). Practical insignificance was demarked by an effect size of r = \|.05\|, depending on the direction of the summary effect.

```{r fail-safe}
fail.safe <- rma(yi = z_psyc_succ,
                 vi = var_z_psyc_succ,
                 slab = Study,
                 data = PDG.remove.na,
                 method = "REML")

fsn(fail.safe, type = "General", target = 0.05)
```

## Sensitivity Analysis Omitting Testori (2019b)

As an exploratory step, a sensitivity analysis was conducted by running a random-effects meta-analytic model after omitting the results of Testori (2019b). As can be seen in the funnel plot, this is the only (a) negative effect size and (b) effect size outside of the demarked bounds of the funnel plot.

```{r sensitivity testori}
omit.testori2019b <- PDG.remove.na[-c(7), ]

psyc.success.omit <- metacor(cor = Psychopathy_Succ, 
                        n = n_effect,
                        studlab = Study,
                        data = omit.testori2019b,
                        fixed = FALSE,
                        random = TRUE,
                        method.tau = "REML",
                        method.random.ci = "HK",
                        title = "Psychopathy and Success in the PDG")

summary(psyc.success.omit)
```

The results of this analysis suggest that psychopathy is significantly associated with success in the PDG, *r* = .117, 95% CI \[.017, .214\], *p* = .030. Interestingly, between-study heterogeneity functionally "disappears", with a non-significant Cochran's *Q* test, *Q*(5) = 3.67, *p* = .598, and a $\tau$^2^ of 0.

## Exploratory Analysis: One-shot vs. Iterated

Although meta-regression should not be used when there are fewer than 10 studies in a meta-analytic model (Deeks et al., 2023), I will be conducting an exploratory analysis to examine the parameter estimates of the psychopathy-success relationship in one-shot vs. iterated games.

```{r exploratory moderator}
psyc.success <- metacor(cor = Psychopathy_Succ, 
                 n = n_effect,
                 studlab = Study,
                 data = PDG.remove.na,
                 fixed = FALSE,
                 random = TRUE,
                 method.tau = "REML",
                 method.random.ci = "HK",
                 title = "Psychopathy and Success in the PDG",
                 subgroup = Iterated)

summary(psyc.success)
```

While there is little power to detect differences (and limited studies in the one-shot condition), it is interesting that the meta-analytic estimate of the correlation between psychopathy-success is higher in one-shot studies (*r* = .1440) than in iterated studies (*r* = .0525).
