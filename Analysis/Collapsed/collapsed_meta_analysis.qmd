---
title: "Collapsed Model"
author: "Liam Doyle"
format: html
embed-resources: true
editor: visual
---

## Description

This document is a summary of the analyses performed for the collapsed meta-analytic model conducted in Study 1 of my dissertation. This model examined the relationship between psychopathy and exploitative behavior collapsed across all economic games under study. The economic games being studied in this research were as follows:

-   Prisoner's Dilemma Game (PDG)
-   Public Goods Game (PGG)
-   Dictator Game (DG)
-   Ultimatum Game (UG)
-   Trust Game (TG)

This model utilized a multi-level or hierarchical approach in order to deal with the issue of nested effect sizes (i.e., some studies contributing more than one effect size to the analysis).

## Loading Packages

All relevant packages loaded below.

```{r library, message=FALSE}
library(tidyverse)
library(meta)
library(metafor)
library(devtools)
library(esc)
library(effectsize)
library(psych)
library(dmetar)
library(readxl)
```

## Loading Data

Loading the excel spreadsheet with all study information.

```{r data import}
All <- read_excel("All.xlsx")
```

## Recoding Variables as Factors

Recoding relevant variables (i.e., moderators in the analysis) as factors.

```{r recode}
All$Sample_Source <- factor(All$Sample_Source,
                            levels = c(0, 1, 2, 3),
                            labels = c("Community", "Student", "Inmate", "Mixed"))

All$Publication_Status <- factor(All$Publication_Status,
                                 levels = c(0, 1),
                                 labels = c("Unpublished", "Published"))

All$Measure_coded <- factor(All$Measure_coded,
                            levels = c(0, 1, 2, 3, 4, 5, 6),
                            labels = c("SRP", "PCL", "PPI", 
                                       "LSRP", "TriPM", "SD3",
                                       "DD"))

All$Game_type <- factor(All$Game_type,
                        levels = c(0, 1, 2, 3, 4, 5, 6),
                        labels = c("All", "PGG", "DG", "UG-P", "UG-R", "TG-Trustor",
                                   "TG-Trustee"))

All$Iterated <- factor(All$Iterated,
                       levels = c(0, 1),
                       labels = c("One-shot", "Iterated"))

All$Perceived_Opponent <- factor(All$Perceived_Opponent,
                                 levels = c(0, 1),
                                 labels = c("No", "Yes"))

All$Social_role <- factor(All$Social_role,
                          levels = c(0, 1),
                          labels = c("Close member", "Distant member"))

All$Incentives <- factor(All$Incentives,
                         levels = c(0, 1, 2),
                         labels = c("Not incentivized", "Participation incentivized",
                                    "Performance incentivized"))

All$Experimental_Manipulation <- factor(All$Experimental_Manipulation,
                                        levels = c(0, 1),
                                        labels = c("No manipulation", 
                                                   "Manipulation"))

All$Feedback <- factor(All$Feedback,
                       levels = c(0, 1),
                       labels = c("No feedback", "Feedback"))

All$Strategy <- factor(All$Strategy,
                       levels = c(0, 1, 2, 3, 4, 5),
                       labels = c("Live opponent", "TFT", "TF2T",
                                  "Neutral/Seesaw", "Random",
                                  "Replicator"))

All$Leniency <- factor(All$Leniency,
                       levels = c(0, 1, 2),
                       labels = c("Lenient", "Neutral", "Harsh"))
```

## Computing Additional Variables

Performing mathematical computations to compute relevant variables such as Fisher's *z*, as well as it's standard error and variance.

```{r transform}
All <- All %>%
  mutate(z_psyc_coop = 0.5 * log((1 + Psych_Coop)/(1 - Psych_Coop)))

All <- All %>%
  mutate(z_se_psyc_coop = 1/(sqrt(n_effect - 3)))

All <- All %>%
  mutate(var_z_psyc_coop = (z_se_psyc_coop^2))
```

## Multi-Level Random-Effects Model of Psychopathy on Cooperation Across Economic Games

As noted in the heading and the project description, this model utilizes a multi-level or hierarchical approach to modelling the relationship between psychopathy and cooperation in economic games. This is due to the fact that several of the studies included in the meta-analysis contributed more than one effect size to the analysis (e.g., a study using more than one measure of psychopathy). The contemporary recommendation for addressing this issue is to use a multi-level approach to cluster these data points - rather than simply aggregating them and ignoring potential differences between the effects.

In addition, I have made the decision to use a random-effects model for the meta-analyses in this dissertation. The fixed-effects model makes untenable assumptions about the sources of variance in the effect sizes (i.e., assuming that all effect sizes are from a homogenous population, and the only source of variance in effect sizes is from sampling error). The assumptions of a fixed-effects model also stand in contradiction to the intended moderator analyses which make up a substantial portion of our later analyses.

### Creation of the Model

```{r model}
full.model <- rma.mv(yi = z_psyc_coop,
                     V = var_z_psyc_coop,
                     slab = Study,
                     data = All,
                     random = ~ 1 | Study/Effect_size_ID,
                     test = "t",
                     method = "REML")

summary(full.model)

convert_z2r(-.1276)
```

As we can see, the model estimates that the meta-analytic effect size is *r* = .127, 95% CI \[-.159, -.096\]. Cochran's *Q* statistic was significant, indicating potential between-study heterogeneity.

### Distribution of Variance Across Levels

This function, provided by the package *dmetar*, calculates a multilevel version of I2. Level 1 variance corresponds to sampling error variance. Level 2 variance corresponds to within-study (or cluster) variance. Level 3 variance corresponds to between-study variance.

```{r variance}
i2 <- var.comp(full.model)

plot(i2)
```

In our case, roughly 67.1% of the variation between effect sizes can be attributed to within-study (50.26% of the total variation) or between-study heterogeneity (16.84%). The remaining variance has been attributed to sampling error. This *I*^2^ value is considered to represent a moderate-to-substantial amount of heterogeneity.

### Comparing Models

Examining the fit of a simple random-effects meta-analytic model vs. the multi-level random-effects meta-analytic model. This suggests that the fit of the multi-level random-effects model is marginally better than a simple random-effects model. Thus, we will retain the multi-level model. Theoretically, the three-level structure also better represents the data-generating process.

```{r}
l3.removed <- rma.mv(yi = z_psyc_coop,
                     V = var_z_psyc_coop,
                     slab = Study,
                     data = All,
                     random = ~ 1 | Study/Effect_size_ID,
                     test = "t",
                     method = "REML",
                     sigma2 = c(0, NA))

anova(full.model, l3.removed)
```

### Forest Plot of the Collapsed Model

Note: This plot had to be generated using a simple random-effects meta-analytic model, as there is not a package available in *R* to accommodate the creation of forest plots for multi-level meta-analytic models.

```{r forest plot, echo=FALSE, warning=FALSE}
psyc_coop_random <- metacor(cor = Psych_Coop,
                            n = n_effect,
                            studlab = Study,
                            data = All,
                            fixed = FALSE,
                            random = TRUE,
                            method.tau = "REML",
                            method.random.ci = "HK",
                            title = "Psychopathy and Cooperation Across All Games")

forestplot <- meta::forest(psyc_coop_random,
                           sortvar = Psych_Coop,
                           prediction = TRUE,
                           print.tau2 = TRUE,
                           width = 800,
                           file = "forest.jpeg")

knitr::include_graphics("forest.jpeg")
```

## Tests of Publication Bias

In this section, I examined the possibility of publication bias via the visual examination of funnel plots, Egger's regression test, a simple meta-regression which regressed publication status (as a binary factor) on effect size, and PET-PEESE methods.

### Funnel Plot

While this funnel plot is asymmetric, it should be noted that we are collapsing across different economic games here. Asymmetry in a funnel plot has several possible sources (Page et al., 2020):

-   Publication bias and other reporting biases;
-   Variability in methodological quality;
-   Between-study heterogeneity;
-   Artefactual variance (i.e., via the correlation between effect sizes and their standard error)
-   Chance

Given the fact that we know, at minimum, that there is substantial between-study heterogeneity due to a variety of games being represented, it is likely that this accounts for the asymmetry in the funnel plot. In other words, we are representing 7 different distributions of effects (from a fixed-effects perspective) within one funnel plot.

```{r funnel}
meta::funnel(psyc_coop_random,
             xlim = c(-1, 1),
             studlab = FALSE)

title("Funnel Plot of the Effect Sizes for the Collapsed Meta-Analytic Model")
```

### Egger's Regression Test

Egger's regression test (Egger et al., 1997) is a method that provides a statistical test of asymmetry in a funnel plot. It involves the prediction of standardized effect sizes on their precision.

The intercept (i.e., $\beta_0$) is of primary interest, representing the asymmetry of the plot. NHST can be used to test the null hypothesis that the intercept is equal to zero, indicating symmetry in the funnel plot. Conversely, a rejected null hypothesis for the intercept indicates asymmetry in the funnel plot - potentially due to publication bias.

The slope (i.e., $\beta_1$) represents the effect size adjusted for precision. It is expected to be consistent with the overall meta-analytic effect.

Reported below is the significance test and parameter estimate for the intercept.

```{r egger}
metabias(psyc_coop_random,
         method.bias = "linreg")
```

### Publication Status as a Moderator of Effect Size

Simple meta-regression was used to predict effect sizes from publication status as a factor (i.e., 0 = unpublished, 1 = published).

```{r moderator publication status}
mod.model.pub <- rma.mv(yi = z_psyc_coop,
                        V = var_z_psyc_coop,
                        slab = Study,
                        data = All,
                        random = ~ 1 | Study/Effect_size_ID,
                        test = "t",
                        method = "REML",
                        mods = ~ Publication_Status)


summary(mod.model.pub)
```

### PET & PEESE Analyses

The PET (Precision-Effect Test) and PEESE (Precision-Effect Estimate with Standard Error) are two analytical methods designed to address publication bias in meta-analyses.

The former, PET analysis, is designed to determine the degree to which small-study effects and publication bias impact a meta-analysis. To do so, effect sizes are predicted from their standard errors. This analysis is used to provide evidence that a genuine effect exists beyond the effect of small-study effects/publication bias.

#### PET Analysis

The key element of the PET analysis is the estimate for $\beta_0$ or the intercept of the regression. According to the logic for PET analysis, a non-significant intercept indicates that no genuine effect occurs beyond small-study effects/publication bias. In contrast, a significant intercept provides evidence for a true effect.

That being said, one potential issue with both PET and PEESE analysis is the assumption that there is a homogenous distribution underlying effect sizes. Conceptually, this is similar to the assumption underlying fixed-effects meta-analyses (https://willgervais.com/blog/2015/6/25/putting-pet-peese-to-the-test-1). While this is problematic in and of itself, we will proceed using the PET-PEESE method.

```{r PET}
pet_model <- rma.mv(yi = z_psyc_coop,
                     V = var_z_psyc_coop,
                    mods = ~ z_se_psyc_coop,
                     slab = Study,
                     data = All,
                     random = ~ 1 | Study/Effect_size_ID,
                     test = "t",
                     method = "REML")

summary(pet_model)
```

As seen in the output, the intercept in the model is significant. This indicates that a true effect exists. According to the PET-PEESE method, we should proceed to the PEESE step of modelling.

#### PEESE Analysis

PEESE analysis aims to produce a precise estimate of the true effect size after accounting for sampling error, small-study effects, and publication bias. It does so by creating a regression where effect sizes are predicted by the squared standard errors of studies.

Again, the primary parameter estimate of interest is $\beta_0$, which indicates the estimate of the true effect size (after adjusting for sampling error, small-study effects, and publication bias). Significance tests are interpreted such that a significant intercept indicates a significant meta-analytic effect size.

A secondary parameter estimate of interest is $\beta_1$, which indicates the relationship between effect sizes and their variances. When this is significant, there is evidence that effect sizes are systematically related to the estimate of their precision (i.e., their variance), which may suggest the presence of small-study effects or publication bias.

More particularly, a positive slope indicates that larger variances (with larger standard errors, and, oftentimes, smaller sample sizes) report larger effect sizes. This is representative of the typical “small-study effect” and indicates potential publication bias.

In contrast, a negative slope suggests that smaller studies report smaller effect sizes and larger studies report larger effect sizes. This may indicate other sources of bias or sources of heterogeneity impacting the results.

```{r PEESE}
peese_model <- rma.mv(yi = z_psyc_coop,
                     V = var_z_psyc_coop,
                    mods = ~ var_z_psyc_coop,
                     slab = Study,
                     data = All,
                     random = ~ 1 | Study/Effect_size_ID,
                     test = "t",
                     method = "REML")

summary(peese_model)

convert_z2r(-.1647)
```

As seen in the model output, the PEESE method indicates that the true effect size is *r* = -.163 and significant at *p* < .0001.

The slope coefficient indicates a positive, significant relationship between effect sizes and their variances (see above). Given the fact that it is a positive relationship, it indicates publication bias according to the logic of the model. However, the most likely reason for this result is a violation of one of the assumptions of the analyses - the fact that effect sizes come from a homogeneous distribution of effects. As we know, the collapsed model instead represents effect sizes drawn from several different distributions. We know this both from our knowledge of existing between-studies heterogeneity (i.e., via different economic games) as well as a more broad understanding of the tenability of fixed vs. random effects meta-analyses. Furthermore, PET-PEESE analyses has been known to perform poorly as between-study heterogeneity increases, and this meta-analytic model has an *I*^2^ value of 67.1%.

To illustrate the slope estimated by the model visually, we will plot the regression.

```{r PEESE plot}
plot(
  All$var_z_psyc_coop, All$z_psyc_coop,
  xlab = "Variance of Effect Sizes",
  ylab = "Effect Sizes",
  main = "PEESE Analysis of Psychopathy-Cooperation Meta-Analysis"
)
abline(a = coef(peese_model)[1], b = coef(peese_model)[2], col = "blue")
```

### Fail-Safe *N*

A variation of Orwin's (1983) fail-safe N test was used to indicate the number of studies with null effects needed to reduce the effect size to a practically irrelevant size (Viechtbauer, 2010). Practical insignificance was demarked by an effect size of r = \|.05\|, depending on the direction of the summary effect.

```{r fail-safe}
fail.safe <- rma(yi = z_psyc_coop,
                 vi = var_z_psyc_coop,
                 slab = Study,
                 data = All,
                 test = "t",
                 method = "REML")

fsn(fail.safe, type = "General", target = 0.05)
```

## Moderator Analysis

Afterwards, a series of moderators were tested. The list of moderators was generated *a priori* using the Affordance-Based Framework of Prosocial Behavior, original ideas, and ideas from previous meta-analyses on psychopathy and cooperation (e.g., Thielmann et al., 2020). In-depth reasoning for the selection of moderators is provided in the dissertation package.

Multiple meta-regression was not used in our study due to constraints arising from (a) a smaller numbers of studies being used (b) the large number of moderators being considered (Higgins & Thompson, 2004). As such, our study explored individual moderators with simple meta-regression analysis. For each model where there were over 10 studies (Deeks et al., 2023), moderators of interest were examined individually using simple meta-regression.

As the current model had 98 effect sizes, simple meta-regression was used to test the list of moderators. The results are shown below.

### *N* as a Moderator of Effect Size

```{r moderator n}
mod.model.n <- rma.mv(yi = z_psyc_coop,
                             V = var_z_psyc_coop,
                             slab = Study,
                             data = All,
                             random = ~ 1 | Study/Effect_size_ID,
                             test = "t",
                             method = "REML",
                             mods = ~ n_effect)


summary(mod.model.n)
```

### Sex Composition as a Moderator of Effect Size

Note: sex composition was a variable represented as the proportion of participants who identified as female in the study.

```{r moderator sex}
mod.model.sex <- rma.mv(yi = z_psyc_coop,
                        V = var_z_psyc_coop,
                        slab = Study,
                        data = All,
                        random = ~ 1 | Study/Effect_size_ID,
                        test = "t",
                        method = "REML",
                        mods = ~ Sex_female)

summary(mod.model.sex)
```

### Mean Age as a Moderator of Effect Size

```{r moderator age}
mod.model.age <- rma.mv(yi = z_psyc_coop,
                        V = var_z_psyc_coop,
                        slab = Study,
                        data = All,
                        random = ~ 1 | Study/Effect_size_ID,
                        test = "t",
                        method = "REML",
                        mods = ~ Age)

summary(mod.model.age)
```

### Sample Source as a Moderator of Effect Size

Note: Studies were coded such that:

-   0 = Community;
-   1 = Student;
-   2 = Inmate;
-   3 = Mixed

```{r moderator sample source}
mod.model.sample <- rma.mv(yi = z_psyc_coop,
                           V = var_z_psyc_coop,
                           slab = Study,
                           data = All,
                           random = ~ 1 | Study/Effect_size_ID,
                           test = "t",
                           method = "REML",
                           mods = ~ Sample_Source)

summary(mod.model.sample)
```

### Psychopathy Measure as a Moderator of Effect Size

Note: Studies were coded such that:

-   0 = SRP;
-   1 = PCL;
-   2 = PPI;
-   3 = LSRP;
-   4 = TriPM;
-   5 = SD3;
-   6 = DD

```{r moderator psychopathy measure}
mod.model.measure <- rma.mv(yi = z_psyc_coop,
                            V = var_z_psyc_coop,
                            slab = Study,
                            data = All,
                            random = ~ 1 | Study/Effect_size_ID,
                            test = "t",
                            method = "REML",
                            mods = ~ Measure_coded)

summary(mod.model.measure)
```

### Economic Game as a Moderator of Effect Size

Note: Studies were coded such that:

-   0 = PDG;
-   1 = PGG;
-   2 = DG;
-   3 = UG-P;
-   4 = UG-R;
-   5 = TG-P;
-   6 = TG-R

```{r moderator game type}
mod.model.game <- rma.mv(yi = z_psyc_coop,
                            V = var_z_psyc_coop,
                            slab = Study,
                            data = All,
                            random = ~ 1 | Study/Effect_size_ID,
                            test = "t",
                            method = "REML",
                            mods = ~ Game_type)

summary(mod.model.game)
```

### One-Shot vs. Iterated as a Moderator of Effect Size

Note: Studies were coded such that:

-   0 = one-shot;
-   1 = iterated

```{r moderator iterated}
mod.model.iterated <- rma.mv(yi = z_psyc_coop,
                             V = var_z_psyc_coop,
                             slab = Study,
                             data = All,
                             random = ~ 1 | Study/Effect_size_ID,
                             test = "t",
                             method = "REML",
                             mods = ~ Iterated)

summary(mod.model.iterated)
```

### Perceived Opponent as a Moderator of Effect Size

Note: Studies were coded such that:

-   0 = participants were *not* led to believe that their opponent was human;
-   1 = participants *were* led to believe that their opponent was human

```{r moderator perceived opponent}
mod.model.perceive <- rma.mv(yi = z_psyc_coop,
                             V = var_z_psyc_coop,
                             slab = Study,
                             data = All,
                             random = ~ 1 | Study/Effect_size_ID,
                             test = "t",
                             method = "REML",
                             mods = ~ Perceived_Opponent)

summary(mod.model.perceive)
```

### Social Role of Opponent as a Moderator of Effect Size

Note: Studies were coded such that:

-   0 = The participants' perceived opponent was a close member of their social circle (e.g., friend, in-group member);
-   1 = The participants' perceived opponent was a distant member of their social circle (e.g., stranger, out-group member)

```{r moderator social role}
mod.model.social <- rma.mv(yi = z_psyc_coop,
                           V = var_z_psyc_coop,
                           slab = Study,
                           data = All,
                           random = ~ 1 | Study/Effect_size_ID,
                           test = "t",
                           method = "REML",
                           mods = ~ Social_role)

summary(mod.model.social)
```

### Incentive Structure as a Moderator of Effect Size

Note: Studies were coded such that:

-   0 = The study was not incentivized;
-   1 = Participation was incentivized;
-   2 = Performance was incentivized

```{r moderator incentive}
mod.model.incentive <- rma.mv(yi = z_psyc_coop,
                              V = var_z_psyc_coop,
                              slab = Study,
                              data = All,
                              random = ~ 1 | Study/Effect_size_ID,
                              test = "t",
                              method = "REML",
                              mods = ~ Incentives)

summary(mod.model.incentive)
```

### Feedback as a Moderator of Effect Size

Note: Studies were coded such that:

-   0 = Participants did not receive feedback on their choices (i.e., what their opponent chose to do, what the outcome was);
-   1 = Participants did receive feedback on their choices

```{r moderator feedback}
mod.model.feedback <- rma.mv(yi = z_psyc_coop,
                             V = var_z_psyc_coop,
                             slab = Study,
                             data = All,
                             random = ~ 1 | Study/Effect_size_ID,
                             test = "t",
                             method = "REML",
                             mods = ~ Feedback)

summary(mod.model.feedback)
```

### Opponent Strategy as a Moderator of Effect Size

Note: Studies were coded such that:

-   0 = Live opponent;
-   1 = TFT;
-   2 = TF2T;
-   3 = Neutral/SeeSaw;
-   4 = Random;
-   5 = Replicator

```{r moderator opponent strategy}
mod.model.strategy <- rma.mv(yi = z_psyc_coop,
                             V = var_z_psyc_coop,
                             slab = Study,
                             data = All,
                             random = ~ 1 | Study/Effect_size_ID,
                             test = "t",
                             method = "REML",
                             mods = ~ Strategy)

summary(mod.model.strategy)
```

### Opponent Leniency as a Moderator of Effect Size

Note: Studies were coded such that:

-   0 = Lenient (e.g., Always Cooperate, TF2T, TF3T);
-   1 = Neutral (e.g., TFT, Win-Stay, Lose-Shift);
-   2 = Harsh (e.g., Always Defect, Grim, 2TFT)

```{r moderator opponent leniency}
mod.model.leniency <- rma.mv(yi = z_psyc_coop,
                             V = var_z_psyc_coop,
                             slab = Study,
                             data = All,
                             random = ~ 1 | Study/Effect_size_ID,
                             test = "t",
                             method = "REML",
                             mods = ~ Leniency)

summary(mod.model.leniency)
```
